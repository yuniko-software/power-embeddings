{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# BGE-M3 ONNX Conversion Research Results\n",
                "\n",
                "This notebook documents our research results for converting the BGE-M3 model to ONNX format, with three key achievements:\n",
                "\n",
                "1. Conversion of BGE-M3 model (dense, sparse, and ColBERT outputs) from FlagEmbedding to ONNX\n",
                "2. Conversion of BGE-M3 tokenizer from Hugging Face to ONNX using ONNX Extensions\n",
                "3. Verification of identical behavior between ONNX and the original FlagEmbedding implementation\n",
                "\n",
                "These conversions allow using BGE-M3 in any language supporting ONNX Runtime (C#, Java, etc.), with identical results to the FlagEmbedding implementation."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Import Required Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install onnx onnxruntime onnxruntime-extensions numpy torch transformers FlagEmbedding"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import onnx\n",
                "import onnxruntime as ort\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from onnxruntime_extensions import gen_processing_models, get_library_path\n",
                "from transformers import AutoTokenizer\n",
                "from FlagEmbedding.inference.embedder.encoder_only.m3 import M3Embedder\n",
                "import os"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Converting BGE-M3 Model to ONNX\n",
                "\n",
                "First, we convert the BGE-M3 model from FlagEmbedding to ONNX with all three outputs (dense, sparse, ColBERT)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "class BGE_M3_ONNX_Wrapper(nn.Module):\n",
                "    \"\"\"Wrapper class to make BGE-M3 compatible with ONNX export\"\"\"\n",
                "    def __init__(self, m3_embedder):\n",
                "        super().__init__()\n",
                "        self.m3_model = m3_embedder.model\n",
                "        \n",
                "    def forward(self, input_ids, attention_mask):\n",
                "        # Call the M3 model with all output types\n",
                "        outputs = self.m3_model({\n",
                "            'input_ids': input_ids, \n",
                "            'attention_mask': attention_mask\n",
                "        }, \n",
                "        return_dense=True,\n",
                "        return_sparse=True, \n",
                "        return_colbert_vecs=True)\n",
                "        \n",
                "        return (\n",
                "            outputs['dense_vecs'],      # Dense embeddings\n",
                "            outputs['sparse_vecs'],     # Sparse weights  \n",
                "            outputs['colbert_vecs']     # ColBERT vectors\n",
                "        )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "def export_bge_m3_standard_format(model_name_or_path=\"BAAI/bge-m3\", output_path=\"bge_m3_model.onnx\"):\n",
                "    \"\"\"Export BGE-M3 model in standard ONNX format: model.onnx + model.onnx_data\"\"\"\n",
                "    print(f\"Loading BGE-M3 model from {model_name_or_path}\")\n",
                "    \n",
                "    # Load the model\n",
                "    embedder = M3Embedder(\n",
                "        model_name_or_path=model_name_or_path,\n",
                "        use_fp16=False,  # Use FP32 for ONNX export\n",
                "        normalize_embeddings=True\n",
                "    )\n",
                "    \n",
                "    # Wrap the model\n",
                "    onnx_model = BGE_M3_ONNX_Wrapper(embedder)\n",
                "    onnx_model.eval()\n",
                "    \n",
                "    # Create dummy input\n",
                "    dummy_input_ids = torch.randint(0, 1000, (1, 512), dtype=torch.long)\n",
                "    dummy_attention_mask = torch.ones(1, 512, dtype=torch.long)\n",
                "    \n",
                "    print(\"Exporting model to ONNX...\")\n",
                "    \n",
                "    # Export to ONNX\n",
                "    torch.onnx.export(\n",
                "        onnx_model,\n",
                "        (dummy_input_ids, dummy_attention_mask),\n",
                "        output_path,\n",
                "        input_names=['input_ids', 'attention_mask'],\n",
                "        output_names=['dense_embeddings', 'sparse_weights', 'colbert_vectors'],\n",
                "        dynamic_axes={\n",
                "            'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
                "            'attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
                "            'dense_embeddings': {0: 'batch_size'},\n",
                "            'sparse_weights': {0: 'batch_size', 1: 'sequence_length'},\n",
                "            'colbert_vectors': {0: 'batch_size', 1: 'sequence_length'}\n",
                "        },\n",
                "        opset_version=20,\n",
                "        export_params=True\n",
                "    )\n",
                "    \n",
                "    print(f\"Model exported to: {output_path}\")\n",
                "    \n",
                "    # Load the exported model and save with external data format\n",
                "    print(\"Converting to external data format...\")\n",
                "    \n",
                "    model = onnx.load(output_path)\n",
                "    \n",
                "    # Get directory and filename parts\n",
                "    output_dir = os.path.dirname(output_path)\n",
                "    base_filename = os.path.basename(output_path)\n",
                "    data_filename = base_filename.replace('.onnx', '.onnx_data')\n",
                "    data_path = os.path.join(output_dir, data_filename)\n",
                "    \n",
                "    onnx.save_model(\n",
                "        model, \n",
                "        output_path,\n",
                "        save_as_external_data=True,\n",
                "        all_tensors_to_one_file=True,\n",
                "        location=data_filename\n",
                "    )\n",
                "    \n",
                "    print(f\"✅ Standard format export completed!\")\n",
                "    print(f\"   Model graph: {output_path}\")\n",
                "    print(f\"   Model data:  {data_path}\")\n",
                "    \n",
                "    return output_path, data_path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create onnx directory if it doesn't exist\n",
                "onnx_dir = \"./onnx\"\n",
                "if not os.path.exists(onnx_dir):\n",
                "    os.makedirs(onnx_dir)\n",
                "    \n",
                "# Set export path\n",
                "bge_m3_onnx_path = os.path.join(onnx_dir, \"bge_m3_model.onnx\")\n",
                "bge_m3_data_path = os.path.join(onnx_dir, \"bge_m3_model.onnx_data\")\n",
                "\n",
                "# Export model if it doesn't exist\n",
                "if not os.path.exists(bge_m3_onnx_path) or not os.path.exists(bge_m3_data_path):\n",
                "    bge_m3_onnx_path, bge_m3_data_path = export_bge_m3_standard_format(output_path=bge_m3_onnx_path)\n",
                "else:\n",
                "    print(f\"Using existing ONNX model at {bge_m3_onnx_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Converting BGE-M3 Tokenizer to ONNX\n",
                "\n",
                "Next, we convert the BGE-M3 tokenizer from Hugging Face to ONNX using ONNX Extensions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": [
                "def export_tokenizer_to_onnx(model_name=\"BAAI/bge-m3\", tokenizer_path=\"onnx/bge_m3_tokenizer.onnx\"):\n",
                "    \"\"\"Export the tokenizer to ONNX format\"\"\"\n",
                "    print(f\"Loading tokenizer from {model_name}\")\n",
                "    hf_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "    \n",
                "    if not os.path.exists(tokenizer_path):\n",
                "        print(f\"Generating ONNX tokenizer at {tokenizer_path}\")\n",
                "        tokenizer_model = gen_processing_models(hf_tokenizer, pre_kwargs={}, post_kwargs={})[0]\n",
                "        with open(tokenizer_path, \"wb\") as f:\n",
                "            f.write(tokenizer_model.SerializeToString())\n",
                "        print(f\"✅ Tokenizer exported to {tokenizer_path}\")\n",
                "    else:\n",
                "        print(f\"Using existing tokenizer at {tokenizer_path}\")\n",
                "    \n",
                "    return hf_tokenizer, tokenizer_path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export tokenizer\n",
                "tokenizer_path = os.path.join(onnx_dir, \"bge_m3_tokenizer.onnx\")\n",
                "hf_tokenizer, tokenizer_path = export_tokenizer_to_onnx(tokenizer_path=tokenizer_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Comparing ONNX Implementation with FlagEmbedding\n",
                "\n",
                "Now we compare the ONNX implementation with the original FlagEmbedding to verify identical behavior."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": [
                "def convert_tokenizer_outputs(tokens, token_indices):\n",
                "    \"\"\"Convert tokenizer outputs to model input format\"\"\"\n",
                "    # Pair tokens with their indices and sort by position\n",
                "    token_pairs = list(zip(token_indices, tokens))\n",
                "    token_pairs.sort()  # Sort by position (token_indices)\n",
                "    \n",
                "    # Get ordered tokens\n",
                "    ordered_tokens = [pair[1] for pair in token_pairs]\n",
                "    \n",
                "    # Create input_ids and attention_mask\n",
                "    input_ids = np.array([ordered_tokens], dtype=np.int64)\n",
                "    attention_mask = np.ones_like(input_ids, dtype=np.int64)\n",
                "    \n",
                "    return input_ids, attention_mask"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": [
                "class OnnxBGEM3Embedder:\n",
                "    \"\"\"BGE-M3 embedder using ONNX tokenizer and model\"\"\"\n",
                "    \n",
                "    def __init__(self, tokenizer_path, model_path):\n",
                "        \"\"\"Initialize the embedder with ONNX tokenizer and model\"\"\"\n",
                "        # Initialize tokenizer session\n",
                "        sess_options = ort.SessionOptions()\n",
                "        sess_options.register_custom_ops_library(get_library_path())\n",
                "        self.tokenizer_session = ort.InferenceSession(\n",
                "            tokenizer_path,\n",
                "            sess_options=sess_options,\n",
                "            providers=['CPUExecutionProvider']\n",
                "        )\n",
                "        \n",
                "        # Initialize model session\n",
                "        self.model_session = ort.InferenceSession(\n",
                "            model_path,\n",
                "            providers=['CPUExecutionProvider']\n",
                "        )\n",
                "        \n",
                "        # Special token IDs for sparse weights filtering\n",
                "        self.special_token_ids = {0, 1, 2, 3}\n",
                "    \n",
                "    def encode(self, text, return_dense=True, return_sparse=False, return_colbert_vecs=False):\n",
                "        \"\"\"Generate embeddings for the input text\"\"\"\n",
                "        # Tokenize the input\n",
                "        tokenizer_outputs = self.tokenizer_session.run(None, {\"inputs\": np.array([text])})\n",
                "        tokens, _, token_indices = tokenizer_outputs\n",
                "        \n",
                "        # Convert to model input format\n",
                "        input_ids, attention_mask = convert_tokenizer_outputs(tokens, token_indices)\n",
                "        \n",
                "        # Generate embeddings\n",
                "        model_outputs = self.model_session.run(None, {\n",
                "            \"input_ids\": input_ids,\n",
                "            \"attention_mask\": attention_mask\n",
                "        })\n",
                "        \n",
                "        # Process outputs\n",
                "        result = {}\n",
                "        \n",
                "        # ONNX outputs: dense_embeddings, sparse_weights, colbert_vectors\n",
                "        dense_embeddings, sparse_weights, colbert_vectors = model_outputs\n",
                "        \n",
                "        # Process dense embeddings\n",
                "        if return_dense:\n",
                "            result[\"dense_vecs\"] = dense_embeddings[0]  # First dimension is batch\n",
                "        \n",
                "        # Process sparse weights\n",
                "        if return_sparse:\n",
                "            # Convert to dictionary format like FlagEmbedding\n",
                "            sparse_dict = {}\n",
                "            for i, token_id in enumerate(input_ids[0]):\n",
                "                if attention_mask[0, i] == 1 and token_id not in self.special_token_ids:\n",
                "                    weight = sparse_weights[0, i]  # [batch, seq_len]\n",
                "                    if weight > 0:\n",
                "                        token_id_int = int(token_id)\n",
                "                        sparse_dict[token_id_int] = max(sparse_dict.get(token_id_int, 0), float(weight))\n",
                "            \n",
                "            result[\"lexical_weights\"] = sparse_dict\n",
                "        \n",
                "        # Process ColBERT vectors\n",
                "        if return_colbert_vecs:\n",
                "            # Convert to list format like FlagEmbedding\n",
                "            colbert_list = []\n",
                "            for i in range(colbert_vectors.shape[1]):  # Iterate over sequence length\n",
                "                if attention_mask[0, i] == 1:  # Only include non-padding tokens\n",
                "                    colbert_list.append(colbert_vectors[0, i])\n",
                "            \n",
                "            result[\"colbert_vecs\"] = colbert_list\n",
                "        \n",
                "        return result"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compare_embeddings(text):\n",
                "    \"\"\"Compare embeddings from ONNX vs original FlagEmbedding\"\"\"\n",
                "    print(f\"Comparing embeddings for: '{text}'\")\n",
                "    \n",
                "    # Load original FlagEmbedding model\n",
                "    flag_embedder = M3Embedder(\n",
                "        model_name_or_path=\"BAAI/bge-m3\",\n",
                "        normalize_embeddings=True\n",
                "    )\n",
                "    \n",
                "    # Load our ONNX implementation\n",
                "    onnx_embedder = OnnxBGEM3Embedder(\n",
                "        tokenizer_path=os.path.join(onnx_dir, \"bge_m3_tokenizer.onnx\"),\n",
                "        model_path=os.path.join(onnx_dir, \"bge_m3_model.onnx\")\n",
                "    )\n",
                "    \n",
                "    # Generate embeddings with both implementations\n",
                "    flag_outputs = flag_embedder.encode(\n",
                "        text,\n",
                "        return_dense=True,\n",
                "        return_sparse=True,\n",
                "        return_colbert_vecs=True\n",
                "    )\n",
                "    \n",
                "    onnx_outputs = onnx_embedder.encode(\n",
                "        text,\n",
                "        return_dense=True,\n",
                "        return_sparse=True,\n",
                "        return_colbert_vecs=True\n",
                "    )\n",
                "    \n",
                "    # Compare dense embeddings\n",
                "    flag_dense = flag_outputs[\"dense_vecs\"]\n",
                "    onnx_dense = onnx_outputs[\"dense_vecs\"]\n",
                "    \n",
                "    print(\"\\n=== DENSE EMBEDDING COMPARISON ===\")\n",
                "    print(f\"FlagEmbedding shape: {flag_dense.shape}, ONNX shape: {onnx_dense.shape}\")\n",
                "    print(f\"First 10 values (Flag): {flag_dense[:10]}\")\n",
                "    print(f\"First 10 values (ONNX): {onnx_dense[:10]}\")\n",
                "    \n",
                "    # Compute cosine similarity for dense\n",
                "    dense_similarity = np.dot(flag_dense, onnx_dense) / (\n",
                "        np.linalg.norm(flag_dense) * np.linalg.norm(onnx_dense)\n",
                "    )\n",
                "    print(f\"Dense cosine similarity: {dense_similarity:.10f}\")\n",
                "    dense_diff = np.abs(flag_dense - onnx_dense).max()\n",
                "    print(f\"Maximum element-wise difference: {dense_diff:.10f}\")\n",
                "    \n",
                "    # Compare sparse weights\n",
                "    flag_sparse = flag_outputs[\"lexical_weights\"]\n",
                "    onnx_sparse = onnx_outputs[\"lexical_weights\"]\n",
                "    \n",
                "    print(\"\\n=== SPARSE WEIGHTS COMPARISON ===\")\n",
                "    print(f\"FlagEmbedding tokens: {len(flag_sparse)}, ONNX tokens: {len(onnx_sparse)}\")\n",
                "    \n",
                "    # Compare top tokens\n",
                "    flag_top = sorted(flag_sparse.items(), key=lambda x: x[1], reverse=True)[:5]\n",
                "    onnx_top = sorted(onnx_sparse.items(), key=lambda x: x[1], reverse=True)[:5]\n",
                "    \n",
                "    print(\"Top 5 tokens (Flag):\")\n",
                "    for token_id, weight in flag_top:\n",
                "        print(f\"  {token_id}: {weight:.6f}\")\n",
                "        \n",
                "    print(\"Top 5 tokens (ONNX):\")\n",
                "    for token_id, weight in onnx_top:\n",
                "        print(f\"  {token_id}: {weight:.6f}\")\n",
                "    \n",
                "    # Compare ColBERT vectors\n",
                "    flag_colbert = flag_outputs[\"colbert_vecs\"]\n",
                "    onnx_colbert = onnx_outputs[\"colbert_vecs\"]\n",
                "    \n",
                "    print(\"\\n=== COLBERT VECTORS COMPARISON ===\")\n",
                "    print(f\"FlagEmbedding vectors: {len(flag_colbert)}, ONNX vectors: {len(onnx_colbert)}\")\n",
                "    \n",
                "    if len(flag_colbert) > 0 and len(onnx_colbert) > 0:\n",
                "        print(f\"First vector dimension (Flag): {len(flag_colbert[0])}\")\n",
                "        print(f\"First vector dimension (ONNX): {len(onnx_colbert[0])}\")\n",
                "        \n",
                "        # Compare first vectors\n",
                "        flag_first = flag_colbert[0]\n",
                "        onnx_first = onnx_colbert[0]\n",
                "        \n",
                "        print(f\"First vector, first 10 values (Flag): {flag_first[:10]}\")\n",
                "        print(f\"First vector, first 10 values (ONNX): {onnx_first[:10]}\")\n",
                "        \n",
                "        # Compute cosine similarity for first vectors\n",
                "        colbert_similarity = np.dot(flag_first, onnx_first) / (\n",
                "            np.linalg.norm(flag_first) * np.linalg.norm(onnx_first)\n",
                "        )\n",
                "        print(f\"First vector cosine similarity: {colbert_similarity:.10f}\")\n",
                "    \n",
                "    # Overall assessment\n",
                "    if dense_similarity > 0.9999 and len(flag_sparse) == len(onnx_sparse):\n",
                "        print(\"\\n✅ CONCLUSION: The ONNX and FlagEmbedding outputs match very closely!\")\n",
                "    else:\n",
                "        print(\"\\n⚠️ CONCLUSION: There are some differences between the ONNX and FlagEmbedding outputs.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test with a diverse multilingual text\n",
                "test_text = \"A test text! Texto de prueba! Текст для теста! 測試文字! Testtext! Testez le texte! Сынақ мәтіні! Тестни текст! परीक्षण पाठ! Kiểm tra văn bản!\"\n",
                "compare_embeddings(test_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusions\n",
                "\n",
                "Our research has successfully achieved the following:\n",
                "\n",
                "1. **Complete BGE-M3 Model Conversion**: We successfully converted the BGE-M3 model from FlagEmbedding to ONNX format, preserving all three output types (dense, sparse, and ColBERT vectors).\n",
                "\n",
                "2. **Tokenizer Conversion**: We also converted the BGE-M3 tokenizer from Hugging Face to ONNX format using ONNX Extensions, enabling seamless text preprocessing in any ONNX-supported language.\n",
                "\n",
                "3. **Identical Behavior**: Our comparison tests demonstrate that the ONNX implementation produces essentially identical outputs to the original FlagEmbedding implementation, with minimal differences due to floating-point precision.\n",
                "\n",
                "These conversions make BGE-M3 available for use in cross-platform applications, particularly in languages like C# and Java, while maintaining the full functionality and accuracy of the original Python implementation."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
